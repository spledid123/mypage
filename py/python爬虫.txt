1、网络爬虫与信息提取（网址：url大概）
（1）requests库：
【1】request（'method'（即GET/PUT/HEAD/POST/PATCH/delete/OPTIONS等），url，**kwargs）
是构造一个请求，其他方法的基础，
 【2】 get(url，params=None，其他**kwargs):获取HTML网页的主要方法，
r=。。。.get()返回Response对象（向网页提交Request对象）
r.status_code  http请求的返回状态200/404（以及其它）    
  r.text HTTP响应内容的字符串，即url对应的页面内容
r.encoding   从HTTP heading中猜测的相应内容编码形式      
r. apparent_encoding从内容中分析出的响应内容编码方式（备选编码方式）
r.content   HTTP相应内容二进制形式r.url    r.headers  r.request.xxx
若header中不存在charest（服务器对编码方式不存在限制），则r.encoding返回ISO-8859-1（默认）
此时需要修改r.encoding(系统默认使用解析r的编码)
【3】**kwargs（13个） params：字典/字节序列，作为参数增加到url中 http://xxx?param
date 字典/字节序列/文件对象，作为Request内容
json:JSON格式数据，Request内容    headers：字典，HTTP定制头
cookies：字典/CokieJar，Request中的cookies    auth：元组，支持HTTP认证功能
files：字典，传输文件     timeout：设定超时时间
proxies：字典，设定访问代理服务器，可以增加登录认证
allow_redirects:True/False,默认True，重定向开关
stream：True/False,默认True，获取内容立即下载开关
verify：True/False,默认True,认证SSL证书开关     cert：本地SSL证书路径
【4】head(url，**kwargs):获取HTML网页的头信息  ，返回一个对象
再用r.headers显示，r.text为空  
post(url,data=None,json=None,**kwargs):向HTML网页提交POST请求（向URL对应资源添加新数据）
r=requests.post('URL',date=xxx)
put(url,date=None,**kwargs):向HTML网页提交PUT请求（存储并覆盖）
patch（url,date=None,**kwargs）：向HTML网页提交局部修改请求
delete(url,**kwargs):向HTML网页提交删除请求（全删）
（2）爬取网页通用代码框架：异常（都是requests.xxx）：requests.ConnectionError 网络连接错误
HTTPError：HTTP解析错误   URLRequired:URL缺失
TooManyRedirects超过最大重定向次数   ConnectTimeout:连接服务器超时
Timeout：请求URL超时
处理r.raise_for_status()若不是200，都产生异常requests.HTTPError
框架： import requests     def getHTMLText(url):  try:   r=requests.get(url,timeout=30)
r.raise_for_status()    r.encoding=r.apparent_encoding    return r.text
except：  return '产生异常'
if _name_=='_main_':  xxx
（3）HTTP协议：超文本传输协议，基于‘请求与响应’模式的无状态的应用层协议，
采用URL作为定位标识，URL:http://host(主机域名或IP地址)[:port]（端口号，默认80）[path]（主机内地址）
HTTP协议对资源有操作功能，requests库的函数与其相对应(不止上述这么多)（但HTTP协议中是大写）
2、网络爬虫的规范：
爬取网页（Requests）<爬取网站（Scrapy）<搜索引擎爬取全网（速度与大小）
带来问题       Robots协议  网站告知哪些网页可以爬取，网站根目录的robots.txt
#注释，*代表所有，/代表根目录，
遵守方式：类人行为可不参考
'user-agent':'Moaill/5.0'/'Chrome/10.0'可修改
3、给搜索引擎提供关键词，通过api，
网络图片的爬取与存储  url/picture.jpg   path=存储路径    url=xxx    r=requests.get(url)
with open(path,'wb') as:    f.write(r.content)    f.close()         文件名称：url.split[-1]  
查询ip地址，检查text最好限制地址


4、BeautifulSoup库
(1)BeautifulSoup库是解析、遍历、维护’标签树‘的功能库（比如HTML/XML文件）
比如p标签   <p(名字) class="title"(属性，键值对构成)>\n........\n</p(结束)>
(2)引用方式：from bs4 import BeautifulSoup/import bs4
HTML文件<->标签树<->BeautifulSoup类 看作等价
from bs4 import BeautifulSoup
soup=BeautifulSoup("<html>date<html>"(即标签树或HTML文档)，"html.parser"(html解析器))
或者soup=BeautifulSoup(open("D://demo.html")(html文件)，"html.parser"(html解析器))
#实际应该是类型转换#若是import bs4，则用bs4.BeautifulSoup
(3)解析器1、bs4的html解析器如上，安装bs4库即可使用；
2、lxml的html解析器pip install lxml 使用BeautifulSoup（mk,'lxml'）
3、lxml的XML解析器 pip install lxml (mk,'xml')
4、html5lib的解析器 pip install html5lib  (mk,'html5lib')
(4)BeautifulSoup类基本元素：
tag标签，基本信息单元，以<>与</>表示开头结尾
name 标签名字,<p>...</p>名字为'p',获取<tag>.name
attributes标签属性，字典形式，<tag>.attrs/<>.attrs['href']特定查找
navigablestring 标签内非属性字符串<>...</>中字符串，<tag>.string
comment 标签内字符串注释部分，一种特殊Comment类型，用<tag>.string引用，但type不同
（5）逐层引用 soup.tag（如p）.name   （只能返回第一个p标签）
(5.5)查找  <tag>.find_all(name,attrs,recursive=True，string=None)
返回列表类型，存储查找结果（标签）
name：（标签名字，'a'/['a','b']/True（所有）/re.compile（'a'）(只显示含'a'的标签)）
attrs：属性检索，只输入属性值'value'（一定要遵守顺序，在name后，否则无法区分）
/属性键值对只能用key=value而不用':'
recursive:布尔值，默认True，是否对所有子孙进行检索
string：字符串区域检索字符串，必须精确输入，（除非用正则表达式）返回字符串的列表（不是标签）
<tag>(...)等价于<tag>.find_all(...)
  基于find_all的拓展,参数与find_all一样：<>.find()只返回一个结果，字符串类型
<>.find_parents()先辈节点搜索，返回列表
<>.find_parent()先辈节点，一个结果，字符串
<>.find_next_siblings()后续平行，列表
<>.find_next_sibling()后续平行，一个结果，字符串
<>.find_previous_siblings()前序平行，列表
<>.find_previous_sibling()前序平行，一个结果，字符串
（6）遍历  下行/上行/平行，下行/平行遍历不一定是标签，可能是navigablestring
下行：.content将<tag>所有儿子节点存入列表（包括'\n'）；.children子节点的迭代类型，用于循环遍历儿子节点
.descendants子孙节点的迭代类型，用于循环遍历
from child in  <tag>.children
上行：.parent上一级/.parents所有先辈的迭代・类型，用于循环
soup.parent is None
for parent is  Soup.a.parents:  if parent is None:  print(parent)  else:print(parent.name)    
平行：.next_sibling按照HTML文本下一个平行标签；.previous_sibling 上一个
.next_sibling 迭代类型  返回后续所有  .previous_sibling迭代，返回前面所有
（7）基于bs4的html输出：  soup.prettify（）可以为标签增加换行符，方便print打印出来
5、信息提取
（1）信息标记  标记信息类别，形成信息组织结构，
HTML格式，是www的主要信息组织方式，能将超文本信息（图片视频等）嵌入文本中，通过预定义的<>...</>
标签形式组织不同类型的信息
信息标记形式：XML（HTML的发展，html被包括）<>..</>或者<p.../>(无内容)注释用<!...>/
JSON,有数据类型的键值对，可以嵌套使用，key：{key1:value}/
YAML   无数据类型键值对，用 缩进 表达所属（嵌套）关系，‘-’表达并列，|表示整块数据#表示注释
XML繁琐但拓展性好 主要用于Internet
JSON适合程序处理（js）简洁，信息有类型，不能注释，能直接用于程序
用于移动应用云端和节点的信息通信，程序接口处理
 YAML信息无类型，可读性好，文本信息比例高，有注释易读，用于各类系统配置文件
（2）信息提取：完整解析标记形式，提取关键信息，速度慢
直接搜索关键信息，不太准确
既要解析又要有文本搜索：形成soup->查找遍历

对齐/中英文，用chr（12288）中文空格填充


5、re库与正则表达式
（1）正则表达式  ：pn、pyn、pyin、pyihn->p(y|yi|yih)?n 简洁， 
表示一组字符串特征py、pypyyy...->py+      1~10个字符，没有p ->[^p]{1,10}(可以是A~Z)
pyn/pin->p[yi]n     p{：3}n->n,pn,ppn,pppn    ^[a-z]+$表示所有a~z
语法     ：字符+操作符   ’.‘表示任何单个字符（除了换行符）  '[]'字符集（[abc]/[a-z]/[^abc]）
'*'前一个字符0次到无限次扩展      '+'前一个字符1次到无限次扩展       '?'前一个字符0次与1次扩展
'|'左右表达式任意一个   {m}扩展前一个字符m次   {m，n}扩展前一个字符[m,n]次
^匹配字符串开头（^ab表示ab，且ab在字符串开头）  $匹配字符串结尾  （）分组标记，内部只能使用'|'
\d数字，表示0~9        \w表示0~9/a~z/A~Z/_
多应用于字符串匹配 
（2）功能函数
re库是标准库  raw string表示正则表达式，表示没有转义符的字符串，表示为r'正则表达式'
也能用string表示正则表达式，\用\\代替
函数：search（pattern(正则表达式),string（字符串）,flags=0（控制标记））
搜索匹配正则表达式的  第一个  位置，返回match对象
flag：  re.I忽略大小写    re.M^可以匹配每行的开始部分    re.S使‘.’可以匹配所有字符
match（pattern，string，flags=0）从字符串开始起匹配正则表达式，返回match对象
要对match是否为空做出判断，if match：即可
findall（pattern，string，flags=0）搜索字符串，以列表类型返回全部能匹配的子串
split（pattern，string，maxsplit=0，flags=0）maxsplit：最大分割/匹配数，剩下的直接输出
将一个字符串按照正则表达式匹配结果进行分割，返回 列表 类型
finditer（pattern，string，flags=0）
搜索字符串，返回 一个 匹配结果的迭代形式，每个迭代形式都是match对象
sub（pattern，repl，string，count=0，flags=0）repl替换匹配字符串的字符串  count最大替换数
在一个字符串中替换所有匹配正则表达式的子串，返回替换后的字符串
（3）使用
函数式：a=re.search(pattern,string)
面向对象：先编译，pat=re.compile(patern)     rst=pat.search(string)
编译：re.compile（pattern,flags=0）将正则表达式的字符串形式转化为正则表达式对象并返回
正则表达式对象的方法与re库功能函数相同，只是没有pattern参数
(4)match对象
属性.string:待匹配的文本；.re匹配时使用的pattern对象；.pos正则表达式搜索文本的开始位置
.endpos正则表达式搜索文本的结束位置；
方法:    .group()/(0)获得匹配后的全部字符串 .group(n)获得第n个括号的匹配结果
   .start()匹配字符串在原始字符串的开始位置 .end()匹配字符串在原始字符串的结束位置->数字
.span()返回（.start(),.end()）元组类型
（5）贪婪匹配与最小匹配
re库默认贪婪匹配（最长）'py.*p'与pyjpjpjpp匹配，默认最长pyjpjpjpp
需要最短字符串->最小匹配操作符
*？/+？/？？/{m,n}?在原来意思上增加最小匹配的含义
（6）实践：搜索与翻页
提交搜索请求，循环获取页面，提取信息，获取url规则
获取html字符串->运用正则表达式->输出
traceback库


Scrapy框架，网站级爬虫
（1）结构：5+2        与reqursts相比，更快，但不太灵活
 数据流：
                1、spiders->middleware->enging->scheduler(requests)
                2、scheduler->enging->middleware->downlouder(requests)->enging
                  middleware->spiders(response)
                 3、spiders->middleware->enging->scheduler(response)/item pipelines(items)
无需编写：enging：控制所有模块间数据流，根据条件触发事件；
                  scheduler:对所有爬取请求进行调度管理
                  downloader:根据请求下载网页  
middleware：downloader middleware：enging，scheduler，downloader间进行用户配置，
                                                                    修改、丢弃，新增请求或响应
                        spider middleware：对请求和爬取项进行再处理，修改/丢弃/新增                                                           
要编写：spiders：解析response，产生scraped item（爬取项），产生额外爬取请求（request）
              item pipelines：以流水线处理items，由一组操作组成， 每个操作都是一个item pipeline类型，
                                         操作包括清理，检验和查重item中HTML数据，将数据存储到数据库        
（2）cmd命令： scrapy -h查看，部分命令只在建立project/spider后才可使用
                        创建新工程：scrapy startproject <name>[dir]
                         创建新爬虫：scrapy genspider [options]<name><domain>
                          运行一个爬虫：scrapy crawl <spider>
                        获得爬虫配置信息：scrapy settings [options]
                          列出project中的所有spider ：scrapy list
                        启动UPL调试命令行：scrapy shell[url]     
(3)1、创建一个新的project以及spider 
                              project目录路径> scrapy startproject project名字：生成project文件夹
                     project路径> scrapy genspider spider名字  网站域名：在spider文件夹里生成一个spider.py文件
1.5、project的文件解析：  
                                           一级：project文件夹
                                           二级：project文件夹：scrapy框架的用户自定义python代码
                                                      scrapy.cfg:部署scrapy爬虫的配置文件（可修改与优化）
                                             三级：_init_.py:初始化脚本(无需修改)
                                                       items.py:  items代码模板（继承类）
                                                        middlewares.py:  middlewares代码模板（继承类）
                                                        pipelines.py:  pipelines代码模板（继承类）
                                                        settings.py:  scrapy爬虫的配置文件
                                                        spiders文件夹：  spiders代码模板目录（继承类）
                                            四级：spider文件夹：_init_.py:初始文件，无需修改
                                                                               _pycache_文件夹  缓存目录，无需修改
2、编写spider
2.5、spider框架解析：class <spider名字>Spider （scrapy.Spider）:  定义了一个类，继承于scrapy.Spider
                             start_urls需要修改为其实网页的url
                                parse函数用于处理响应（response），解析内容，形成字典，并发现新的爬取请求
                                self只是一个对象的标记，不是参数
3、编写itempipeline，优化配置
4、运行    project路径> scrapy crawl spider名字
（4） yield与生成器：不断产生值的函数，比如range
                               用yield语句产生并返回一个值，（语法类似return）
                             函数冻结，局部变量不变，再次被唤醒时在生成新的值 
                              常与循环并用，节省内存
 （5）三大类：Request类，封装有一个Request对象   class scrapy.http.Request()
                                         Request对象表示一个http请求。
                                         类由spider生成，由downloader执行
                                    属性或方法：.url   request对应的url
                                                        .method请求方法，比如'GET'
                                                       .headers字典类型，request的header
                                                        .body：request内容主题，字符
                                                         .meta 用户添加的扩展信息，在Scrapy内部模块传递信息
                                                        .copy()  复制Request
                          Response类封装有一个Response对象 class scrapy.http.Response()
                                                        Response对象表示一个http响应
                                                             由downloader生成，spider处理
                                         属性和方法：.url/.status:HTTP状态码（如200）/.headers/.copy()
                                                            /.body字符串/.flags一组标记/.request 产生该类型对应的Request对象
                         Item类  class scrapy.item.Item()
                                                      Item对象表示从HTML页面中提取的信息内容
                                                    spider生成，itempipeline处理
                                               Item类似于字典类型
（6）html信息提取， scrapy爬虫支持多种信息提取方法
  CSS Selector介绍：<html>.css('a::attr(href)').extract()(标签名称与属性)  提取
（7）实例：建立project与spider->编写spider->编写item pipeline
编写spider意味着修改start_urls,scrapy自动download并形成self与response对象
                          再在parse中对这两个对象进行处理即可
scrapy.Request(url,callback=self.fun)
编写pipeline：编写pipeline代码

                                                           